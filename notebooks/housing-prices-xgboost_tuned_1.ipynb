{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":5407,"databundleVersionId":868283,"sourceType":"competition"}],"dockerImageVersionId":31089,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-19T14:17:01.452870Z","iopub.execute_input":"2025-08-19T14:17:01.453715Z","iopub.status.idle":"2025-08-19T14:17:01.463530Z","shell.execute_reply.started":"2025-08-19T14:17:01.453681Z","shell.execute_reply":"2025-08-19T14:17:01.462663Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/house-prices-advanced-regression-techniques/sample_submission.csv\n/kaggle/input/house-prices-advanced-regression-techniques/data_description.txt\n/kaggle/input/house-prices-advanced-regression-techniques/train.csv\n/kaggle/input/house-prices-advanced-regression-techniques/test.csv\n","output_type":"stream"}],"execution_count":43},{"cell_type":"code","source":"!pip -q install optuna lightgbm catboost","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T16:40:11.383016Z","iopub.execute_input":"2025-08-19T16:40:11.383432Z","iopub.status.idle":"2025-08-19T16:40:15.474962Z","shell.execute_reply.started":"2025-08-19T16:40:11.383397Z","shell.execute_reply":"2025-08-19T16:40:15.473670Z"}},"outputs":[],"execution_count":81},{"cell_type":"code","source":"##Load Data\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import cross_val_score, KFold\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom xgboost import XGBRegressor\n\nTRAIN_PATH = \"/kaggle/input/house-prices-advanced-regression-techniques/train.csv\"\nTEST_PATH = \"/kaggle/input/house-prices-advanced-regression-techniques/test.csv\"\nSAMPLE_PATH = \"/kaggle/input/house-prices-advanced-regression-techniques/sample_submission.csv\"\n\ntrain = pd.read_csv(TRAIN_PATH)\ntest = pd.read_csv(TEST_PATH)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T14:17:01.465612Z","iopub.execute_input":"2025-08-19T14:17:01.465979Z","iopub.status.idle":"2025-08-19T14:17:01.536785Z","shell.execute_reply.started":"2025-08-19T14:17:01.465943Z","shell.execute_reply":"2025-08-19T14:17:01.535774Z"}},"outputs":[],"execution_count":44},{"cell_type":"code","source":"#Split Features\n\ny = train[\"SalePrice\"]\nX = train.drop([\"SalePrice\", \"Id\"], axis=1)\nX_test_final = test.drop([\"Id\"], axis=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T14:17:01.538393Z","iopub.execute_input":"2025-08-19T14:17:01.538740Z","iopub.status.idle":"2025-08-19T14:17:01.551263Z","shell.execute_reply.started":"2025-08-19T14:17:01.538696Z","shell.execute_reply":"2025-08-19T14:17:01.549660Z"}},"outputs":[],"execution_count":45},{"cell_type":"code","source":"#Log-transform target for stability\ny = np.log1p(y)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T14:48:59.974196Z","iopub.execute_input":"2025-08-19T14:48:59.975191Z","iopub.status.idle":"2025-08-19T14:48:59.981045Z","shell.execute_reply.started":"2025-08-19T14:48:59.975153Z","shell.execute_reply":"2025-08-19T14:48:59.979776Z"}},"outputs":[],"execution_count":56},{"cell_type":"code","source":"# ---- Add engineered features to BOTH train and test ----\ndef add_features(df):\n    df = df.copy()\n    # robust helpers (use .get so it won’t crash if a col is missing)\n    df[\"TotalSF\"]     = df.get(\"TotalBsmtSF\", 0) + df.get(\"1stFlrSF\", 0) + df.get(\"2ndFlrSF\", 0)\n    df[\"TotalBath\"]   = df.get(\"FullBath\", 0) + 0.5*df.get(\"HalfBath\", 0) + df.get(\"BsmtFullBath\", 0)\n    df[\"HouseAge\"]    = df.get(\"YrSold\", 0) - df.get(\"YearBuilt\", 0)\n    df[\"RemodAge\"]    = df.get(\"YrSold\", 0) - df.get(\"YearRemodAdd\", 0)\n    df[\"OverallQual_SF\"] = df.get(\"OverallQual\", 0) * df.get(\"GrLivArea\", 0)\n    # simple flags\n    df[\"HasGarage\"] = (df.get(\"GarageArea\", 0) > 0).astype(int)\n    df[\"HasBasement\"] = (df.get(\"TotalBsmtSF\", 0) > 0).astype(int)\n    df[\"HasFireplace\"]= (df.get(\"Fireplaces\", 0) > 0).astype(int)\n    return df\n\nX = add_features(X)\nX_test_final = add_features(X_test_final)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T14:49:04.238897Z","iopub.execute_input":"2025-08-19T14:49:04.239249Z","iopub.status.idle":"2025-08-19T14:49:04.260169Z","shell.execute_reply.started":"2025-08-19T14:49:04.239227Z","shell.execute_reply":"2025-08-19T14:49:04.259046Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in greater\n  return op(a, b)\n/usr/local/lib/python3.11/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in greater\n  return op(a, b)\n","output_type":"stream"}],"execution_count":57},{"cell_type":"code","source":"#Separate Categorical from Numeric\ncategorical = X.select_dtypes(include=[\"object\"]).columns\nnumeric = X.select_dtypes(exclude=[\"object\"]).columns","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T14:49:13.779166Z","iopub.execute_input":"2025-08-19T14:49:13.780141Z","iopub.status.idle":"2025-08-19T14:49:13.788215Z","shell.execute_reply.started":"2025-08-19T14:49:13.780099Z","shell.execute_reply":"2025-08-19T14:49:13.787258Z"}},"outputs":[],"execution_count":58},{"cell_type":"code","source":"#Preproccessor\n\nnumeric_transformer = Pipeline(steps=[\n    (\"imputer\", SimpleImputer(strategy=\"median\"))\n])\n\ncategorical_transformer = Pipeline(steps=[\n    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n])\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        (\"num\", numeric_transformer, numeric),\n        (\"cat\", categorical_transformer, categorical)\n    ]\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T15:01:07.565777Z","iopub.execute_input":"2025-08-19T15:01:07.566171Z","iopub.status.idle":"2025-08-19T15:01:07.573027Z","shell.execute_reply.started":"2025-08-19T15:01:07.566142Z","shell.execute_reply":"2025-08-19T15:01:07.571942Z"}},"outputs":[],"execution_count":67},{"cell_type":"code","source":"#Model XGBoost\n\nmodel = XGBRegressor(\n     n_estimators=1538\n    ,learning_rate=0.01343\n    ,max_depth=4\n    ,subsample=0.6962\n    ,colsample_bytree=0.6221\n    ,reg_alpha=0.108\n    ,reg_lambda=1.145\n    ,min_child_weight=7\n    ,random_state=42\n    ,n_jobs=-1\n    ,tree_method=\"hist\"\n)\n\npipeline = Pipeline(steps=[(\"preprocessor\", preprocessor), \n                          (\"model\", model)])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T18:30:15.847167Z","iopub.status.idle":"2025-08-19T18:30:15.847859Z","shell.execute_reply.started":"2025-08-19T18:30:15.847557Z","shell.execute_reply":"2025-08-19T18:30:15.847583Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Grid Search\nfrom sklearn.model_selection import GridSearchCV, KFold\n\nparam_grid = {\n    \"model__n_estimators\": [300, 600, 900],\n    \"model__max_depth\": [3, 4, 5],\n    \"model__learning_rate\": [0.02, 0.05, 0.08],\n    \"model__subsample\": [0.7, 0.8, 1.0],\n    \"model__colsample_bytree\": [0.7, 0.8, 1.0],\n}\n\ncv = KFold(n_splits=5, shuffle=True, random_state=42)\n\ngrid = GridSearchCV(\n    estimator=pipeline,\n    param_grid=param_grid,\n    scoring=\"neg_root_mean_squared_error\",\n    cv=cv, n_jobs=-1, verbose=1\n)\ngrid.fit(X, y)\nprint(\"Best params:\", grid.best_params_)\nprint(\"Best CV RMSE:\", -grid.best_score_)\nbest_pipeline = grid.best_estimator_\n\nscores = cross_val_score(best_pipeline, X, y, scoring=\"neg_root_mean_squared_error\",\n                        cv=cv, n_jobs=-1)\nprint(f\"CV RMSE (log-price): {-scores.mean():.4f} +/- {scores.std():.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T15:46:44.769654Z","iopub.execute_input":"2025-08-19T15:46:44.770068Z","iopub.status.idle":"2025-08-19T15:59:10.094081Z","shell.execute_reply.started":"2025-08-19T15:46:44.770039Z","shell.execute_reply":"2025-08-19T15:59:10.092883Z"}},"outputs":[{"name":"stdout","text":"Fitting 5 folds for each of 243 candidates, totalling 1215 fits\nBest params: {'model__colsample_bytree': 1.0, 'model__learning_rate': 0.05, 'model__max_depth': 3, 'model__n_estimators': 900, 'model__subsample': 0.7}\nBest CV RMSE: 0.009592154462460738\nCV RMSE (log-price): 0.0096 +/- 0.0012\n","output_type":"stream"}],"execution_count":71},{"cell_type":"code","source":"#Optuna Optimization\n\nimport optuna\nimport numpy as np\nimport pandas as pd\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import ElasticNet\n\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom catboost import CatBoostRegressor\n\nSCORING = \"neg_root_mean_squared_error\" # CV metric (lower RMSE = better)\n\ndef cv_rmse(pipe):\n    scores = cross_val_score(pipe, X, y, cv=cv, scoring=SCORING, n_jobs=-1)\n    return -scores.mean()\n\ndef make_pipeline(model):\n    return Pipeline(steps=[(\"preprocessor\", preprocessor), (\"model\", model)])\n\ndef tune_with_optuna(model_name, n_trials=40, random_state=42):\n    def objective(trial):\n        if model_name == \"xgb\":\n            params = dict(\n                n_estimators=trial.suggest_int(\"n_estimators\", 600, 3000),\n                learning_rate=trial.suggest_float(\"learning_rate\", 0.005, 0.2, log=True),\n                max_depth=trial.suggest_int(\"max_depth\", 2, 10),\n                subsample=trial.suggest_float(\"subsample\", 0.5, 1.0),\n                colsample_bytree=trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n                reg_alpha=trial.suggest_float(\"reg_alpha\", 0.0, 5.0),\n                reg_lambda=trial.suggest_float(\"reg_lambda\", 0.0, 5.0),\n                min_child_weight=trial.suggest_int(\"min_child_weight\", 1, 10),\n                tree_method=\"hist\",\n                n_jobs=-1,\n                random_state=random_state,\n                verbosity=0\n            )\n            model = XGBRegressor(**params)\n        elif model_name == \"lgbm\":\n            params = dict(\n                n_estimators=trial.suggest_int(\"n_estimators\", 800, 5000),\n                learning_rate=trial.suggest_float(\"learning_rate\", 0.005, 0.2, log=True),\n                num_leaves=trial.suggest_int(\"num_leaves\", 16, 256),\n                max_depth=trial.suggest_int(\"max_depth\", -1, 12),\n                subsample=trial.suggest_float(\"subsample\", 0.5, 1.0),\n                colsample_bytree=trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n                min_child_samples=trial.suggest_int(\"min_child_samples\", 5, 50),\n                reg_alpha=trial.suggest_float(\"reg_alpha\", 0.0, 5.0),\n                reg_lambda=trial.suggest_float(\"reg_lambda\", 0.0, 5.0),\n                objective=\"regression\",\n                n_jobs=-1,\n                random_state=random_state,\n                verbose=0\n            )\n            model = LGBMRegressor(**params)\n        elif model_name == \"cat\":\n            params = dict(\n                iterations=trial.suggest_int(\"iterations\", 800, 4000),\n                learning_rate=trial.suggest_float(\"learning_rate\", 0.005, 0.2, log=True),\n                depth=trial.suggest_int(\"depth\", 4, 10),\n                l2_leaf_reg=trial.suggest_float(\"l2_leaf_reg\", 1.0, 10.0),\n                bagging_temperature=trial.suggest_float(\"bagging_temperature\", 0.0, 1.0),\n                loss_function=\"RMSE\",\n                random_state=random_state,\n                verbose=False\n            )\n            model = CatBoostRegressor(**params)\n        elif model_name == \"rf\":\n            params = dict(\n                n_estimators=trial.suggest_int(\"n_estimators\", 400, 1600),\n                max_depth=trial.suggest_int(\"max_depth\", 6, 30),\n                max_features=trial.suggest_categorical(\"max_features\", [\"sqrt\", \"log2\", 0.5, 0.7, 0.9]),\n                min_samples_split=trial.suggest_int(\"min_samples_split\", 2, 10),\n                min_samples_leaf=trial.suggest_int(\"min_samples_leaf\", 1, 5),\n                n_jobs=-1,\n                random_state=random_state,\n            )\n            model = RandomForestRegressor(**params)\n        elif model_name == \"elastic\":\n            params = dict(\n                alpha=trial.suggest_float(\"alpha\", 1e-4, 10.0, log=True),\n                l1_ratio=trial.suggest_float(\"l1_ratio\", 0.0, 1.0),\n                max_iter=20000,\n                random_state=random_state\n            )\n            model = ElasticNet(**params)\n        else:\n            raise ValueError(\"Unknown model_name\")\n\n        pipe = make_pipeline(model)\n        return cv_rmse(pipe)\n\n    study = optuna.create_study(direction=\"minimize\")\n    study.optimize(objective, n_trials=n_trials, show_progress_bar=False)\n    print(f\"{model_name} best RMSE:\", study.best_value)\n    print(f\"{model_name} best params:\", study.best_params)\n\n    # Build the best model/pipeline and refit on all data\n    best_model = (\n        XGBRegressor(tree_method=\"hist\", n_jobs=-1, random_state=42, **study.best_params) if model_name==\"xgb\" else\n        LGBMRegressor(objective=\"regression\", n_jobs=-1, random_state=42, **study.best_params) if model_name==\"lgbm\" else\n        CatBoostRegressor(loss_function=\"RMSE\", random_state=42, verbose=False, **study.best_params) if model_name==\"cat\" else\n        RandomForestRegressor(n_jobs=-1, random_state=42, **study.best_params) if model_name==\"rf\" else\n        ElasticNet(max_iter=20000, random_state=42, **study.best_params)\n    )\n\n    best_pipe = make_pipeline(best_model)\n    best_pipe.fit(X, y)\n    return study.best_value, best_pipe","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T16:54:28.637038Z","iopub.execute_input":"2025-08-19T16:54:28.638076Z","iopub.status.idle":"2025-08-19T16:54:28.657380Z","shell.execute_reply.started":"2025-08-19T16:54:28.638037Z","shell.execute_reply":"2025-08-19T16:54:28.656436Z"}},"outputs":[],"execution_count":85},{"cell_type":"code","source":"#Model Tuning\n\nrmse_xgb, pipe_xgb = tune_with_optuna(\"xgb\", n_trials=2)\nrmse_cat, pipe_cat = tune_with_optuna(\"cat\", n_trials=2)\nrmse_rf, pipe_rf = tune_with_optuna(\"rf\", n_trials=2)\nrmse_en, pipe_en = tune_with_optuna(\"elastic\", n_trials=4) # linear-ish baseline","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T18:31:51.940971Z","iopub.execute_input":"2025-08-19T18:31:51.941448Z","iopub.status.idle":"2025-08-19T18:36:36.258778Z","shell.execute_reply.started":"2025-08-19T18:31:51.941416Z","shell.execute_reply":"2025-08-19T18:36:36.257470Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stderr","text":"[I 2025-08-19 18:31:51,943] A new study created in memory with name: no-name-1185d4ba-2b6d-4bf2-82b0-96a7e5f1038d\n[I 2025-08-19 18:31:56,640] Trial 0 finished with value: 0.01828748125053436 and parameters: {'n_estimators': 1639, 'learning_rate': 0.03729953474357719, 'max_depth': 2, 'subsample': 0.534442600686247, 'colsample_bytree': 0.6657290882384869, 'reg_alpha': 2.691356439065573, 'reg_lambda': 1.3573650869402858, 'min_child_weight': 6}. Best is trial 0 with value: 0.01828748125053436.\n[I 2025-08-19 18:31:58,408] Trial 1 finished with value: 0.01907823700117493 and parameters: {'n_estimators': 1439, 'learning_rate': 0.17315751700933962, 'max_depth': 5, 'subsample': 0.9608284810654639, 'colsample_bytree': 0.5508316432485123, 'reg_alpha': 4.624524458300815, 'reg_lambda': 3.151033550345112, 'min_child_weight': 7}. Best is trial 0 with value: 0.01828748125053436.\n","output_type":"stream"},{"name":"stdout","text":"xgb best RMSE: 0.01828748125053436\nxgb best params: {'n_estimators': 1639, 'learning_rate': 0.03729953474357719, 'max_depth': 2, 'subsample': 0.534442600686247, 'colsample_bytree': 0.6657290882384869, 'reg_alpha': 2.691356439065573, 'reg_lambda': 1.3573650869402858, 'min_child_weight': 6}\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-08-19 18:31:59,294] A new study created in memory with name: no-name-4cb257cb-db37-47af-bb51-9bfa5ea2643e\n[I 2025-08-19 18:32:18,907] Trial 0 finished with value: 0.010262338875935837 and parameters: {'iterations': 1123, 'learning_rate': 0.008455146216847837, 'depth': 4, 'l2_leaf_reg': 7.436269445495387, 'bagging_temperature': 0.7816580795268946}. Best is trial 0 with value: 0.010262338875935837.\n[I 2025-08-19 18:34:10,520] Trial 1 finished with value: 0.00957426284014209 and parameters: {'iterations': 2407, 'learning_rate': 0.03465784217254741, 'depth': 7, 'l2_leaf_reg': 5.486829064854026, 'bagging_temperature': 0.4631358191156607}. Best is trial 1 with value: 0.00957426284014209.\n","output_type":"stream"},{"name":"stdout","text":"cat best RMSE: 0.00957426284014209\ncat best params: {'iterations': 2407, 'learning_rate': 0.03465784217254741, 'depth': 7, 'l2_leaf_reg': 5.486829064854026, 'bagging_temperature': 0.4631358191156607}\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-08-19 18:34:30,808] A new study created in memory with name: no-name-a065d36b-0948-4c52-9570-6fec13128c16\n[I 2025-08-19 18:34:45,321] Trial 0 finished with value: 0.011002217620392569 and parameters: {'n_estimators': 1151, 'max_depth': 11, 'max_features': 'sqrt', 'min_samples_split': 5, 'min_samples_leaf': 1}. Best is trial 0 with value: 0.011002217620392569.\n[I 2025-08-19 18:35:22,314] Trial 1 finished with value: 0.010855702534769543 and parameters: {'n_estimators': 540, 'max_depth': 13, 'max_features': 0.9, 'min_samples_split': 6, 'min_samples_leaf': 5}. Best is trial 1 with value: 0.010855702534769543.\n","output_type":"stream"},{"name":"stdout","text":"rf best RMSE: 0.010855702534769543\nrf best params: {'n_estimators': 540, 'max_depth': 13, 'max_features': 0.9, 'min_samples_split': 6, 'min_samples_leaf': 5}\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-08-19 18:35:36,253] A new study created in memory with name: no-name-ef6f7f37-061e-4d60-b342-5e21bf28b66b\n[I 2025-08-19 18:35:37,083] Trial 0 finished with value: 0.014990282253553425 and parameters: {'alpha': 0.255469995468009, 'l1_ratio': 0.28115274131067136}. Best is trial 0 with value: 0.014990282253553425.\n[I 2025-08-19 18:35:41,313] Trial 1 finished with value: 0.012720202854787768 and parameters: {'alpha': 0.001336671297321851, 'l1_ratio': 0.6919945142237351}. Best is trial 1 with value: 0.012720202854787768.\n[I 2025-08-19 18:35:42,180] Trial 2 finished with value: 0.014317086150148772 and parameters: {'alpha': 0.02892065928862604, 'l1_ratio': 0.24919607780936293}. Best is trial 1 with value: 0.012720202854787768.\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.643e-04, tolerance: 1.064e-04\n  model = cd_fast.enet_coordinate_descent(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_coordinate_descent.py:592: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0004796873916088351, tolerance: 0.00011044632544738668\n  model = cd_fast.sparse_enet_coordinate_descent(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_coordinate_descent.py:592: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.00035769987441849893, tolerance: 0.00010476645828650448\n  model = cd_fast.sparse_enet_coordinate_descent(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_coordinate_descent.py:592: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.00034543638729599146, tolerance: 0.00011393398175582439\n  model = cd_fast.sparse_enet_coordinate_descent(\n[I 2025-08-19 18:36:11,655] Trial 3 finished with value: 0.011328678184941524 and parameters: {'alpha': 0.00010789618989867691, 'l1_ratio': 0.9935030452682905}. Best is trial 3 with value: 0.011328678184941524.\n","output_type":"stream"},{"name":"stdout","text":"elastic best RMSE: 0.011328678184941524\nelastic best params: {'alpha': 0.00010789618989867691, 'l1_ratio': 0.9935030452682905}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_coordinate_descent.py:592: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0003877976531604377, tolerance: 0.00013715157781717628\n  model = cd_fast.sparse_enet_coordinate_descent(\n","output_type":"stream"}],"execution_count":89},{"cell_type":"code","source":"# Predict (in log space) with each tuned model\npreds_log_xgb = pipe_xgb.predict(X_test_final)\npreds_log_cat = pipe_cat.predict(X_test_final)\npreds_log_rf = pipe_rf.predict(X_test_final)\npreds_log_en = pipe_en.predict(X_test_final)\n\n# ==== Ensemble ====\n# Option A: simple average\npreds_log_blend = (preds_log_xgb + preds_log_cat + preds_log_rf + preds_log_en) / 4.0\n\n# Option B: performance-weighted average (better RMSE -> higher weight)\nrmse_list = np.array([rmse_xgb, rmse_cat, rmse_rf, rmse_en], dtype=float)\nweights = (1.0 / (rmse_list ** 2))\nweights = weights / weights.sum()\npreds_log_weighted = (weights[0]*preds_log_xgb +\n                      weights[1]*preds_log_cat + weights[2]*preds_log_rf +\n                      weights[3]*preds_log_en)\n\n# Choose which to use for submission:\nuse_weighted = True\npreds_log_final = preds_log_weighted if use_weighted else preds_log_blend\n\n# Invert log1p to dollars\npreds_final = np.expm1(preds_log_final) \n\n# Clip to non-negative\npreds_final = np.clip(preds_final, 0, None)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T18:36:42.995959Z","iopub.execute_input":"2025-08-19T18:36:42.996324Z","iopub.status.idle":"2025-08-19T18:36:43.416811Z","shell.execute_reply.started":"2025-08-19T18:36:42.996299Z","shell.execute_reply":"2025-08-19T18:36:43.415778Z"}},"outputs":[],"execution_count":90},{"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\n\ncandidates = {\n    \"Baseline XGB\": pipeline,\n    \"Tuned XGB\": best_pipeline,\n    # \"Stacked\": stacked_pipeline, # if you created it\n}\nfor name, pipe in candidates.items():\n    scores = -cross_val_score(pipe, X, y, cv=cv, scoring=\"neg_root_mean_squared_error\", n_jobs=-1)\n    print(f\"{name:15s} CV RMSE: {scores.mean():.5f} (+/- {scores.std():.5f})\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T18:36:50.118080Z","iopub.execute_input":"2025-08-19T18:36:50.118421Z","iopub.status.idle":"2025-08-19T18:37:00.627350Z","shell.execute_reply.started":"2025-08-19T18:36:50.118397Z","shell.execute_reply":"2025-08-19T18:37:00.626390Z"}},"outputs":[{"name":"stdout","text":"Baseline XGB    CV RMSE: 0.00974 (+/- 0.00123)\nTuned XGB       CV RMSE: 0.00959 (+/- 0.00124)\n","output_type":"stream"}],"execution_count":91},{"cell_type":"code","source":"#submission\n\nsub = pd.read_csv(\"/kaggle/input/house-prices-advanced-regression-techniques/sample_submission.csv\")\nsub[\"SalePrice\"] = preds_final\nsub.to_csv(\"submission.csv\", index=False)\nprint(\"Saved submission.csv — Ensemble of XGB + LGBM + CatBoost + RF + ElasticNet\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T18:37:36.803925Z","iopub.execute_input":"2025-08-19T18:37:36.804282Z","iopub.status.idle":"2025-08-19T18:37:36.833062Z","shell.execute_reply.started":"2025-08-19T18:37:36.804257Z","shell.execute_reply":"2025-08-19T18:37:36.831910Z"}},"outputs":[{"name":"stdout","text":"Saved submission.csv — Ensemble of XGB + LGBM + CatBoost + RF + ElasticNet\n","output_type":"stream"}],"execution_count":92}]}